{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#word2vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is equivalent to `demo-word.sh`, `demo-analogy.sh`, `demo-phrases.sh` and `demo-classes.sh` from Google."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download some data, for example: [http://mattmahoney.net/dc/text8.zip](http://mattmahoney.net/dc/text8.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `word2phrase` to group up similar words \"Los Angeles\" to \"Los_Angeles\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a `text8-phrases` that we can use as a better input for `word2vec`.\n",
    "Note that you could easily skip this previous step and use the origial data as input for `word2vec`.\n",
    "\n",
    "Train the model using the `word2phrase` output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file result.txt\n",
      "Vocab size: 667\n",
      "Words in train file: 11837\n"
     ]
    }
   ],
   "source": [
    "word2vec.word2vec('result.txt', 'ress.bin', size=100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That generated a `text8.bin` file containing the word vectors in a binary format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the clustering of the vectors based on the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file ress.bin\n",
      "Vocab size: 4\n",
      "Words in train file: 1713\n"
     ]
    }
   ],
   "source": [
    "word2vec.word2clusters('ress.bin', 'result-clusters.txt', 100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That created a `text8-clusters.txt` with the cluster for every word in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the `word2vec` binary file created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = word2vec.load('ress.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the vocabulaty as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'</s>', u'#EVET', u'i\\xe7in', u'#Evet', u've', u'bu',\n",
       "       u'#MemurSeneDavetTercihEvet', u'#evet',\n",
       "       u'#Hay\\u0131rDemiyoruz\\xc7\\xdcNK\\xdc', u'T\\xfcrkiye', u'bir',\n",
       "       u'aday', u'G\\xfc\\xe7l\\xfc', u'de', u'Bu', u'16', u'ne',\n",
       "       u'#HaydiT\\xfcrkiye', u'1', u'b\\xfcy\\xfck', u'g\\xfc\\xe7l\\xfc',\n",
       "       u'Hem', u'@TurkayMetin:', u'y\\xfczden', u'#HAYIR', u'Referandumda',\n",
       "       u'!!', u'kar\\u015f\\u0131', u'olmaya', u'\\xe7\\u0131k\\u0131nca',\n",
       "       u'\\xe7at\\u0131', u'\\xf6nerirler', u'ak\\u015feneri', u'\\u0130zleyin',\n",
       "       u'i\\xe7in,', u'\\xe7\\u0131kt\\u0131klar\\u0131',\n",
       "       u'ba\\u015fkanl\\u0131\\u011fa', u'meral', u'ikisi', u'utan\\u0131r',\n",
       "       u'istikrarl\\u0131', u'\\xe7\\u0131kar\\u0131p', u'heralde',\n",
       "       u'@ReisciHatice:', u'@disli_merve86:', u',', u'karar\\u0131m\\u0131z',\n",
       "       u'kadar', u'@rali35:', u'\\u0130\\xe7in', u'oyunuz',\n",
       "       u'@osmanll_beyi:', u'rt', u'http\\u2026RT', u'(Oy',\n",
       "       u's\\xfcrecektir.', u'hafta', u'Anket', u'??', u'olacakt\\u0131r',\n",
       "       u'ANKETT', u'DEV', u'ki', u'demokrasi', u'edelim)',\n",
       "       u'https://t.c\\u2026RT', u'olarak', u'sonra', u'kulland\\u0131ktan',\n",
       "       u'da', u'tek', u'diyor.', u'S\\u2026RT', u'Ak', u'#EBS',\n",
       "       u'\\u0130zmir1', u'TEK', u'Memur-Sen', u'sendika', u'15',\n",
       "       u'\\u0130\\u015fte', u'@RT_Erdogan', u'Var', u'diye',\n",
       "       u'https://t.\\u2026RT', u'Hay\\u0131r', u'sonuna', u'@HARUNKARACA1',\n",
       "       u'@Emekheryerde:', u'Say\\u0131n', u'#Emekheryerde',\n",
       "       u'Ba\\u015fkan\\u0131m\\u0131z', u'#BA\\u015e\\u2026RT',\n",
       "       u'#VolkanBa\\u015fkanl\\u0131', u'@EmineNurGunay', u'yapan',\n",
       "       u'hesaplar', u'#HerZamanOdunpazar\\u0131', u'@mdunlu',\n",
       "       u'\\xe7al\\u0131\\u015faca\\u011f\\u0131z', u'i\\xe7inde', u'Yeni',\n",
       "       u'Bizlerde', u'@MemurSenKonf', u'\\xfcyeleri', u'kesinlikle',\n",
       "       u'\\u0130\\xc7\\u0130N', u'Neden', u'a\\xe7\\u0131klad\\u0131',\n",
       "       u'@_aliyalcin_', u'Gelece\\u011fi', u'Uyum',\n",
       "       u'payla\\u015f\\u0131mlar', u'@SosyoRT:', u'baz\\u0131', u'ediyor.',\n",
       "       u'hizmet', u'daha', u'Genel', u'hay\\u0131rc\\u0131lara',\n",
       "       u'salak\\xe7a', u'#\\u0130ddiaEdiyorum', u'hayr\\u0131', u'\\U0001f534',\n",
       "       u'diyecek', u'mu', u'Bir', u'hi\\xe7?', u'\\xdclken', u'de!',\n",
       "       u'hay\\u0131rl\\u0131', u\"hay\\u0131r'\\u0131\", u'Sen',\n",
       "       u'olmayanlar\\u0131n', u'izleyin', u'hi\\xe7bir', u'17', u'Sonra',\n",
       "       u'\"', u'ht\\u2026RT', u'@ersoydede',\n",
       "       u'payla\\u015facaks\\u0131n\\u0131z', u'anlatacak', u'dile', u'dilden',\n",
       "       u'bunu', u'hemde', u'@Halil_Bayrakli:', u'D\\u0130KKAT',\n",
       "       u'L\\xfct\\u2026RT', u'olur', u'GEL', u'\\U0001f534Tek', u'milletin',\n",
       "       u'\\xdclkeye', u'#HepDedi\\u011fimGibi', u'@ekazimkarabekir:',\n",
       "       u'inat', u'millet', u'Hep', u'diyoruz...\\u2026RT',\n",
       "       u'y\\u0131l\\u0131nda', u'@MeralitaRTE:', u'H\\xdcR', u'DAVA',\n",
       "       u'PART\\u0130S\\u0130', u'(H\\xfcda-Par)', u'Referendumda', u'Oyu',\n",
       "       u'Kullanaca\\u011f\\u0131n\\u0131', u'A\\xe7\\u0131klad\\u0131..',\n",
       "       u'Dar\\u0131s\\u0131', u'Saadet', u'partisine', u'ihtimali',\n",
       "       u'Sa\\u011fl\\u0131k', u'@OduncuTimi:', u'istedi\\u011finiz',\n",
       "       u'kesmek', u'\\xf6n\\xfcn\\xfc', u'y\\xfckseli\\u015fin', u'\\u015fiir',\n",
       "       u'okudu\\u011fu', u'1997', u'aral\\u0131k', u'@nazmiakcan:', u'her',\n",
       "       u'birlikte', u'Anlars\\u0131n\\u0131z', u'Hi\\xe7', u'Diyenler',\n",
       "       u'#hay\\u0131r', u'Belki', u'Sefer', u'Dedi\\u011fimizi',\n",
       "       u'kaybedecek', u'@CuneytYonet:', u'BU', u'#AdaletTecelliEdecek',\n",
       "       u'Yok', u'B\\xfcy\\xfck', u\"T\\xfcrkiye'nin\", u'olma', u'Y\\xfcregine',\n",
       "       u'CUMHUR\\u0130YET\\u0130N', u'KIZI', u'#HAYIRRR', u'@yuunusozdemir:',\n",
       "       u'yer', u'g\\xf6revlilerinin', u'Selamun',\n",
       "       u'https://t.co/g9zt8xMZbORT', u'Padi\\u015fah', u'de\\u011fil..',\n",
       "       u'\\u0130nand\\u0131\\u011f\\u0131m', u'Ne', u'kimsenin', u'hukukunun',\n",
       "       u'onun', u'karar', u'y\\xfcz\\xfc\\u011f\\xfcn\\xfc', u'SALTANAT',\n",
       "       u'hak', u'emek\\xe7ilerin', u'T\\xdcRK\\u0130YE', u'@ask4978:',\n",
       "       u'Tarih', u'olan', u'kazand\\u0131\\u011f\\u0131,',\n",
       "       u'g\\xfc\\xe7lendi\\u011fi,', u'Reis', u'#EvetMemurSenNet',\n",
       "       u'Aleyk\\xfcm', u\"Nisan'da\", u'iken', u'\\u015fehzade', u'Ben',\n",
       "       u'vermedi\\u011fi', u'#YenidenB\\xfcy\\xfckT\\xfcrkiye',\n",
       "       u'parma\\u011f\\u0131na', u'ihanetin', u'resmini', u'Kabe',\n",
       "       u'Tanklar\\u0131n', u'Kamu', u'@VecdiYanbaz__:', u'#Memu\\u2026RT',\n",
       "       u'https://t.co/9TAkA86QlJRT', u'Karar\\u0131n\\u0131', u'mermilerin',\n",
       "       u'Ve', u'indi\\u011fi', u'gibi', u'y\\xfcrekler', u'\\xc7ok',\n",
       "       u'bak\\u0131\\u015flar\\u0131', u'Milli', u'ezdi\\u011fi', u'.\\u2026RT',\n",
       "       u'yi\\u011fitler', u'Hay\\u0131rs\\u0131zlara', u'Gen\\xe7lerin',\n",
       "       u'mekanizmas\\u0131nda', u'\\xe7abuk', u'almalar\\u0131',\n",
       "       u'tercihimiz', u'tak\\u2026RT', u'Diyor', u'sistemine',\n",
       "       u'in\\u015fhAllah..\\U0001f61eRT', u'!!!',\n",
       "       u'#Cumhurba\\u015fkanl\\u0131\\u011f\\u0131H\\xfck\\xfcmetSistemi',\n",
       "       u'unuttun', u'gitmez', u'izli', u'Milleti',\n",
       "       u'kar\\u015f\\u0131lan\\u0131yor.', u'@gencmemursen34:',\n",
       "       u'y\\xfcr\\xfctme', u'g\\xfczel', u'G\\xf6z\\xfcmden',\n",
       "       u'sevmiyorlar.\\u0130\\u015fte', u'Karar\\u0131', u'kald\\u0131ran',\n",
       "       u'@sedat_peker', u'Reisi', u'erkini', u'ortadan', u'Tabiki',\n",
       "       u'@suleymannzengin:', u'#evetdiyece\\u011fim\\xe7\\xfcnk\\xfc',\n",
       "       u'ayk\\u0131r\\u0131', u'iradeye', u'@bilgebilgi55:',\n",
       "       u'#EVET\\u2026RT', u'Devlet', u'do\\u011frudan', u'Abd\\xfclhamit',\n",
       "       u'\\u0130stikrar', u'belirlenmesi', u'Bildirdi', u'@06melihgokcek',\n",
       "       u'Kirli', u'B\\xf6l\\xfcyorlar.', u'@Vatan_SancAK',\n",
       "       u'https://t.co/4D8BwyigkWRT', u'alk\\u0131\\u015fl\\u0131yor',\n",
       "       u'Hainler', u'deyince', u'Kopuyor', u'K\\u0131yamet', u'Deyince',\n",
       "       u'ikna', u'https://t.co/40J0kyt8YURT', u'yok', u'diyoruz.',\n",
       "       u'g\\xfcn', u'@deligz:', u'@muhru_mirac_rt:', u'diyorum.', u'94',\n",
       "       u'vatan', u'Bayrak', u'O', u'B\\u0130R', u'@Nesiine', u'diyoruz',\n",
       "       u'Vatan', u'@OzluGonul:', u'\\xdclkeyi', u'emellere',\n",
       "       u'#MemurSeneD\\u2026RT', u'@begnusoylu', u\"'cilerine\", u'diyor',\n",
       "       u'demeye', u'@Adem_C4LIK:', u'Teslim', u'Nisan\\u0131n',\n",
       "       u'Bek\\xe7ilerine', u'Temmuzun', u'olsun..', u'Sel\\xe2m', u'iki',\n",
       "       u'ba\\u015fkan', u'Bug\\xfcn', u'Elhamdulillah', u'!',\n",
       "       u'yar\\u0131nlar', u'VE', u'o', u'diyen', u'#EVETRT', u'Haydi',\n",
       "       u'@mesutoner45:', u\"A\\u011fr\\u0131's\\u0131\", u'Kar\\u0131n',\n",
       "       u'G\\xfcruhun', u'\\u015eER', u'\\xc7ekemeyen',\n",
       "       u'\\U0001f1f9\\U0001f1f7\\u2764\\ufe0f', u'L\\u0130DER\\u0130M\\u0130Z',\n",
       "       u'SECDEL\\u0130', u'ALNI', u'karars\\u0131z', u'diyenlerin',\n",
       "       u'yeniden', u'\\xe7a\\u011fr\\u0131s\\u0131nda', u'#evetRT', u'yazmaya',\n",
       "       u'M\\u0130', u'https://t.co/ZxguYVovfTRT', u'@GunayKaya08:', u'evet',\n",
       "       u'referandumda', u'TOBB', u'#Serdenge\\xe7tilerTakiple\\u015fiyor',\n",
       "       u'#EvetRT', u'Z\\u0130HN\\u0130YET', u'@SerdengectiRT:',\n",
       "       u'BA\\u015eIMIZIN', u'zihniyet', u'BELASI', u'OLDU..',\n",
       "       u'bunlar\\u0131', u'REFERANDUM', u'ile', u'amcayla',\n",
       "       u'konu\\u015ftum', u'GEEL', u'BASALIM', u'LER\\u0130',\n",
       "       u'B\\u0130TS\\u0130N', u'\\xc7APUL', u'dedi', u\"2'sini\", u'ettim',\n",
       "       u\"'evet'\", u'fav\\u0131', u'@RTE_SIIRT1:', u'alay\\u0131m',\n",
       "       u'Hisarc\\u0131kl\\u0131o\\u011flu\\u2019na', u'@Ikzdr:',\n",
       "       u'#MemurSeneDavetTercihEvetRT', u'H\\xfck\\xfcmet', u'Yarg\\u0131',\n",
       "       u'YILDIR', u'Yeni,', u'duyal\\u0131m-:)\\u2026',\n",
       "       u'@UstAkilOyunlar\\u2026RT', u'ya', u'duyu\\u2026RT', u'@tyfnrte:',\n",
       "       u'@bekiservet:', u'@osmanli_ak_1299:', u'@Cimcime874K:',\n",
       "       u'derseniz;', u'senide', u'bulunuyorum.', u'Nisan', u'metroda',\n",
       "       u'R\\u0131fat', u'saltanat\\u0131n\\u0131', u'duble', u'@huseyn5676:',\n",
       "       u'yollar', u'y\\u0131kaca\\u011f\\u0131z\"', u'Osmangazi',\n",
       "       u'@RT\\u2026RT', u'@SEDAT_PEKER', u'80', u'y\\u0131l\\u0131m\\u0131z',\n",
       "       u'ba\\u015fkan\\u0131', u'@Seferiyobaz', u'@TATARKENAN23',\n",
       "       u'@yildirim2555', u'#Haberler', u'mi', u'K\\xf6pr\\xfcs\\xfc',\n",
       "       u'#Akkad\\u0131nlar', u'#Hay\\u0131r', u'Ba\\u015fkan\\u0131',\n",
       "       u'#AkEski\\u015fehir', u'#Akparti', u'de\\u011fil', u'devlet',\n",
       "       u'Mescid-i', u'@akkadn_26:', u'Ayd\\u0131nl\\u0131k', u'EVET',\n",
       "       u'yolu', u'https://t.co/Be0cT\\u2026RT', u'YAHU', u'Marmararay',\n",
       "       u'Eme\\u011fi', u'y\\u0131l', u'diyece\\u011fiz', u'istiklali',\n",
       "       u'Y\\xfczd\\u2026RT', u'\\xfclkemizin', u'Yetmez', u'sat\\u0131rlar',\n",
       "       u'Edi\\u2026RT', u'@OzkanNa\\u2026RT', u'Bilir',\n",
       "       u'Z\\u0130HN\\u0130YET..', u'devam', u'y\\u0131ld\\u0131r',\n",
       "       u'@fideerken:', u'@iykaynarca\\u2026RT', u'@Julide77',\n",
       "       u'@GMakkadinmedya', u'@kilicdarogluk', u'Ge\\xe7ti\\u011fimiz',\n",
       "       u'emek', u'\"Osmanl\\u0131', u'y\\u0131kt\\u0131\\u011f\\u0131m\\u0131z',\n",
       "       u'le', u'edece\\u011fiz.!', u'@sevdamrabbim:', u'#TabikiEVET',\n",
       "       u'Akar', u'3', u'@AbdullahsarGul:', u'\\U0001f605\\U0001f605RT',\n",
       "       u'de\\u011fil,', u'bile', u'olsun...', u'son', u'CHPKK', u'e',\n",
       "       u'#tabiiki', u'-HakEttik', u'\\u0130srail', u'hala', u'@RTE_AYAZ:',\n",
       "       u'Bizim', u'@Kayabey_81', u'@28Yasinturan', u'-Allah\\u015eahit',\n",
       "       u'sabah', u'i\\u015fte', u'bizim', u'-Bekliyoruz', u'-5Y\\u0131l',\n",
       "       u'@Reisci1953:', u'Bunun', u'ruhu', u'Temmuz', u'olsa', u'K\\u0130',\n",
       "       u'alg\\u0131', u'-4200Ki\\u015fi',\n",
       "       u'#Kurum\\u0130ciGelirUzmanl\\u0131\\u011f\\u0131',\n",
       "       u'DERD\\u0130M\\u0130Z', u'DerdimizinDerman\\u0131Sizsiniz',\n",
       "       u'y\\xfcr\\xfcmeye', u'seni', u'biz', u'oldu\\u011fu', u'olsun',\n",
       "       u'@SercanBilgi:', u'bayrak', u'Diktat\\xf6rl\\xfck', u'Y\\u0131kar!',\n",
       "       u'https://t.co/DaHXWRFNMcRT', u'#VatanBayrak\\u0130\\xe7inEVET',\n",
       "       u'\\xf6nde', u'alg\\u0131s\\u0131', u'yapt\\u0131lar%69', u'seferde',\n",
       "       u'istikbal', u\"Karaman'dan\", u'sende', u'Parti', u'AK',\n",
       "       u'@Akgenc1_2023:', u'sabahlar', u'Hay\\u0131rl\\u0131', u'minennevm',\n",
       "       u'beraberlik', u'Nisanda', u'Hayrun', u'\"Essel\\xe2tu',\n",
       "       u'okumu\\u015f...\\u2714', u'ezan\\u0131', u'Sabah', u'Nebevi',\n",
       "       u'\\u2714', u'\\xdclkesi', u'\\xe7al\\u0131\\u015fan,15', u'hizmetleri',\n",
       "       u'ortada', u'yi\\u011fit', u'adamla', u'diyerek', u'Daha',\n",
       "       u'@Kahve_Bahcesi:', u'\\u015febekesinden', u'cevap', u'zaman',\n",
       "       u'B\\u0130RL\\u0130KTE', u'kurgu', u'?', u'HEP', u'diyorsan',\n",
       "       u'T\\xfcrk', u'yine', u'ismini', u'RE\\u0130S', u'@ZehraKl32421689:',\n",
       "       u'#AkParti', u'#RTE', u'\\U0001f4a1\\U0001f4a1\\U0001f4a1\\U0001f4a1',\n",
       "       u'videolarla', u'Sn\\u2026RT', u'@imera_ferah61:', u'icin',\n",
       "       u'Genelkurmay', u'.', u'Y\\xdcZDEN', u'tepki',\n",
       "       u'\\xe7al\\u0131\\u015fmas\\u0131na', u'@_Del\\u2026RT', u'hep',\n",
       "       u'#Hay\\u0131rDemiyoruz\\xc7\\xdcNK\\xdc;', u'\"#EVET',\n",
       "       u'paras\\u0131yla', u'yeni', u'@SSSBBL777:', u'gelecek',\n",
       "       u'cephesine', u'DE', u'Gibi', u'evinde', u'operasyonu.',\n",
       "       u'B\\xfct\\xfcn', u'@SofiZennube:', u'https://t.co/a5kv60JmYTRT',\n",
       "       u'birlik', u'\\xe7\\xfcnki', u'bozmak', u'Diyoruz',\n",
       "       u'TAMBA\\u011eIMSIZ\"T\\xdcRK\\u0130YE\"', u'BAYRAK...i\\xe7in..',\n",
       "       u\"'\\u0131n\", u'ben', u'DEVLET..', u'VATAN..', u'M\\u0130LLET..',\n",
       "       u'olabilir', u'\\U0001f1f9\\U0001f1f7AK', u'TAK\\u0130P', u'ama',\n",
       "       u'#Hay\\u0131rD\\u2026RT', u'\\U0001f1f9\\U0001f1f7BU',\n",
       "       u'\\U0001f1f9\\U0001f1f7TWT\\u0130', u'\\U0001f1f9\\U0001f1f7RTWT',\n",
       "       u'\\u0130nad\\u0131na', u'\\U0001f1f9\\U0001f1f7BE\\u011eEN\\u0130',\n",
       "       u\"yolumuz'dur..\", u'olup', u'a\\u015f\\u0131k',\n",
       "       u'inand\\u0131\\u011f\\u0131m\\u0131z', u'cellad\\u0131na', u'k\\u0131rk',\n",
       "       u'takla', u'atan', u'arkada\\u015f', u'yarsan\\u0131z', u'yap\\u0131n',\n",
       "       u'm\\xfchr\\xfcn\\xfc', u'S\\u0131rf', u'temsil', u'etti\\u011fin',\n",
       "       u'zihniyeti', u'sa\\u2026RT', u'kurulmasin', u'ey', u'yalanla',\n",
       "       u'size', u'art\\u0131r\\u0131rken', u'kinimi', u'\\xf6fkemi,',\n",
       "       u'TAB\\u0130\\u0130', u'edilmesi', u'idam', u'dedemin', u'\\u015eimdi',\n",
       "       u'giymeyen', u'\\u015fapkas\\u0131', u'\\U0001f1f9\\U0001f1f7YAPANLAR',\n",
       "       u'#Karar\\u0131m\\u0131zEVET', u'\\U0001f60e', u'#TurkeySaysYes',\n",
       "       u'namertlere,', u'90',\n",
       "       u'\\U0001f1f9\\U0001f1f7TAK\\u0130PLE\\u015e\\u0130YOR', u'K\\xfcrt',\n",
       "       u'\\U0001f609', u'\\u0130hanet', u'f\\u0131rsat',\n",
       "       u'https://t.co/yJNe3nR\\u2026RT', u'vermemek',\n",
       "       u'#Haz\\u0131rOlT\\xfcrkiyem', u'@23YANILMAZ:',\n",
       "       u'\\U0001f1f9\\U0001f1f7'], \n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or take a look at the whole matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(667, 100)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14333282,  0.15825513, -0.13715845, ...,  0.05456942,\n",
       "         0.10955409,  0.00693387],\n",
       "       [ 0.06664074, -0.06769899,  0.10004526, ...,  0.10045119,\n",
       "        -0.06882237, -0.02533416],\n",
       "       [ 0.05349666, -0.05904634,  0.09227172, ...,  0.09860691,\n",
       "        -0.07739629, -0.01857724],\n",
       "       ..., \n",
       "       [ 0.02696064,  0.0314932 ,  0.02478098, ..., -0.14323677,\n",
       "         0.11688532, -0.10754634],\n",
       "       [ 0.06302015, -0.05156104,  0.09801275, ...,  0.09141515,\n",
       "        -0.06747776, -0.01908772],\n",
       "       [ 0.04681491, -0.06786318,  0.09205017, ...,  0.0919707 ,\n",
       "        -0.07321247, -0.02706222]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retreive the vector of individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['idam'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05424013, -0.06254178,  0.09455197, -0.08326341, -0.05127008,\n",
       "       -0.03213024, -0.02676981, -0.02153507,  0.02765418,  0.01497025])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['idam'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do simple queries to retreive words similar to \"socks\" based on cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 69, 447, 191, 443,  85,  43, 292, 117,   8,   7]),\n",
       " array([ 0.99964538,  0.99963192,  0.99962978,  0.99962612,  0.99962009,\n",
       "         0.99961879,  0.99961694,  0.99961638,  0.99961524,  0.9996137 ]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes, metrics = model.cosine('idam')\n",
    "indexes, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returned a tuple with 2 items:\n",
    "1. numpy array with the indexes of the similar words in the vocabulary\n",
    "2. numpy array with cosine similarity to each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its possible to get the words of those indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'da', u'@akkadn_26:', u'#hay\\u0131r', u'#Akparti', u'Hay\\u0131r',\n",
       "       u'@ReisciHatice:', u'iradeye', u'daha',\n",
       "       u'#Hay\\u0131rDemiyoruz\\xc7\\xdcNK\\xdc', u'#evet'], \n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab[indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a helper function to create a combined response: a numpy [record array](http://docs.scipy.org/doc/numpy/user/basics.rec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.array([(u'da', 0.9996453834842265), (u'@akkadn_26:', 0.9996319174433688),\n",
       " (u'#hay\\u0131r', 0.9996297831994851), (u'#Akparti', 0.9996261227615416),\n",
       " (u'Hay\\u0131r', 0.9996200872714502),\n",
       " (u'@ReisciHatice:', 0.9996187866042381), (u'iradeye', 0.9996169418796403),\n",
       " (u'daha', 0.999616378188327),\n",
       " (u'#Hay\\u0131rDemiyoruz\\xc7\\xdcNK\\xdc', 0.9996152368835952),\n",
       " (u'#evet', 0.9996137044813816)], \n",
       "          dtype=[(u'word', '<U78'), (u'metric', '<f8')])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_response(indexes, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is easy to make that numpy array a pure python response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'da', 0.9996453834842265),\n",
       " (u'@akkadn_26:', 0.9996319174433688),\n",
       " (u'#hay\\u0131r', 0.9996297831994851),\n",
       " (u'#Akparti', 0.9996261227615416),\n",
       " (u'Hay\\u0131r', 0.9996200872714502),\n",
       " (u'@ReisciHatice:', 0.9996187866042381),\n",
       " (u'iradeye', 0.9996169418796403),\n",
       " (u'daha', 0.999616378188327),\n",
       " (u'#Hay\\u0131rDemiyoruz\\xc7\\xdcNK\\xdc', 0.9996152368835952),\n",
       " (u'#evet', 0.9996137044813816)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_response(indexes, metrics).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we trained the model with the output of `word2phrase` we can ask for similarity of \"phrases\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'news', 0.9972785387251313),\n",
       " (u'yay\\u0131mlanan', 0.9971057734839921),\n",
       " (u'telefonla', 0.9970603290236499),\n",
       " (u'konvoy', 0.9967531315324603),\n",
       " (u'yetkilisi', 0.996713066815481),\n",
       " (u'toplant\\u0131n\\u0131n', 0.9966796915519048),\n",
       " (u'verdi\\u011fi', 0.9965941774654892),\n",
       " (u'ingiliz', 0.9965921817610062),\n",
       " (u'brown', 0.9965833434147313),\n",
       " (u'h\\xfcseyin', 0.9965249465320618)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes, metrics = model.cosine('yetkilisi')\n",
    "model.generate_response(indexes, metrics).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its possible to do more complex queries like analogies such as: `king - man + woman = queen` \n",
    "This method returns the same as `cosine` the indexes of the words in the vocab and the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'news', 0.9972785387251313),\n",
       " (u'yay\\u0131mlanan', 0.9971057734839921),\n",
       " (u'telefonla', 0.9970603290236499),\n",
       " (u'konvoy', 0.9967531315324603),\n",
       " (u'yetkilisi', 0.996713066815481),\n",
       " (u'toplant\\u0131n\\u0131n', 0.9966796915519048),\n",
       " (u'verdi\\u011fi', 0.9965941774654892),\n",
       " (u'ingiliz', 0.9965921817610062),\n",
       " (u'brown', 0.9965833434147313),\n",
       " (u'h\\xfcseyin', 0.9965249465320618)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_response(indexes, metrics).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some errors were detected !\n    Line #3 (got 1 columns instead of 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c3f97697bbc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result-clusters.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/unstblecrsr/anaconda/lib/python2.7/site-packages/word2vec/io.pyc\u001b[0m in \u001b[0;36mload_clusters\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mword\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     '''\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordClusters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/unstblecrsr/anaconda/lib/python2.7/site-packages/word2vec/wordclusters.pyc\u001b[0m in \u001b[0;36mfrom_text\u001b[0;34m(cls, fname)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/unstblecrsr/anaconda/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows)\u001b[0m\n\u001b[1;32m   1767\u001b[0m             \u001b[0;31m# Raise an exception ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minvalid_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1769\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1770\u001b[0m             \u001b[0;31m# Issue a warning ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Some errors were detected !\n    Line #3 (got 1 columns instead of 1)"
     ]
    }
   ],
   "source": [
    "clusters = word2vec.load_clusters('result-clusters.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see get the cluster number for individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ce855a26f543>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dog'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clusters' is not defined"
     ]
    }
   ],
   "source": [
    "clusters['dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see get all the words grouped on an specific cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-e394b137a939>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclusters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_words_on_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clusters' is not defined"
     ]
    }
   ],
   "source": [
    "clusters.get_words_on_cluster(90).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-30558004960f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclusters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_words_on_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clusters' is not defined"
     ]
    }
   ],
   "source": [
    "clusters.get_words_on_cluster(90)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add the clusters to the word2vec model and generate a response that includes the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-212711f54948>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clusters' is not defined"
     ]
    }
   ],
   "source": [
    "model.clusters = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'paris'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d6c0af43b7d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paris'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'germany'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'france'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/unstblecrsr/anaconda/lib/python2.7/site-packages/word2vec/wordvectors.pyc\u001b[0m in \u001b[0;36manalogy\u001b[0;34m(self, pos, neg, n)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/unstblecrsr/anaconda/lib/python2.7/site-packages/word2vec/wordvectors.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/unstblecrsr/anaconda/lib/python2.7/site-packages/word2vec/wordvectors.pyc\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \"\"\"\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/unstblecrsr/anaconda/lib/python2.7/site-packages/word2vec/wordvectors.pyc\u001b[0m in \u001b[0;36mix\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mon\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \"\"\"\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_hash\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'paris'"
     ]
    }
   ],
   "source": [
    "indexes, metrics = model.analogy(pos=['paris', 'germany'], neg=['france'], n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'#Hay\\u0131rDemiyoruz\\xc7\\xdcNK\\xdc', 0.9999983076674978),\n",
       " (u'#EVET', 0.9999979252163659),\n",
       " (u'VE', 0.999997727445976),\n",
       " (u'15', 0.9999974709044099),\n",
       " (u'#EVET\\u2026RT', 0.9999974676813989),\n",
       " (u'#evet', 0.9999973989631359),\n",
       " (u'i\\xe7in', 0.9999973316027865),\n",
       " (u'diyoruz.', 0.9999972452130969),\n",
       " (u'B\\u0130R', 0.9999971474304682),\n",
       " (u'@kkaadder:', 0.9999969153788237)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_response(indexes, metrics).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
